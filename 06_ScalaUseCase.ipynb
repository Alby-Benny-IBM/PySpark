{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alby-Benny-IBM/PySpark/blob/main/06_ScalaUseCase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "S9oMeEbwX5LS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fWl5VIkzaSvr"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/scala/scala/releases/download/v2.12.18/scala-2.12.18.deb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dpkg -i scala-2.12.18.deb"
      ],
      "metadata": {
        "id": "bCfxbbjnabuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e39b776-1cb2-4ca0-c788-a9f1f248bc24"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package scala.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack scala-2.12.18.deb ...\n",
            "Unpacking scala (2.12.18-400) ...\n",
            "Setting up scala (2.12.18-400) ...\n",
            "Creating system group: scala\n",
            "Creating system user: scala in scala with scala daemon-user and shell /bin/false\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "1cFtfuOXXq-W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set environment variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "ifa1rH3nXt-X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "zXIKBQ-IXwTs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "gSTO_zHTXxzh",
        "outputId": "dffa5f81-29dc-4f28-cdce-d7070cd9d38c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7a2a15724250>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4e47019e14ba:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R csv/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDMXTJwFX0Y5",
        "outputId": "565cea1e-f287-43f4-d848-d9c3c2285b79"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "csv/:\n",
            "customers.csv  offices.csv\t orders.csv    productlines.csv\n",
            "employees.csv  orderdetails.csv  payments.csv  products.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ClassicModelsApp.scala\n",
        "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.sql.expressions.Window\n",
        "\n",
        "object ClassicModelsApp {\n",
        "\n",
        "  def main(args: Array[String]): Unit = {\n",
        "    val spark = SparkSession.builder()\n",
        "      .appName(\"ClassicModels Analytics\")\n",
        "      .master(\"local[*]\")\n",
        "      .getOrCreate()\n",
        "\n",
        "    import spark.implicits._\n",
        "\n",
        "    val basePath = \"/content/csv\"\n",
        "\n",
        "    // Load CSVs into DataFrames\n",
        "    val customersDF     = loadCSV(spark, s\"$basePath/customers.csv\")\n",
        "    val employeesDF     = loadCSV(spark, s\"$basePath/employees.csv\")\n",
        "    val officesDF       = loadCSV(spark, s\"$basePath/offices.csv\")\n",
        "    val orderDetailsDF  = loadCSV(spark, s\"$basePath/orderdetails.csv\")\n",
        "    val ordersDF        = loadCSV(spark, s\"$basePath/orders.csv\")\n",
        "    val paymentsDF      = loadCSV(spark, s\"$basePath/payments.csv\")\n",
        "    val productLinesDF  = loadCSV(spark, s\"$basePath/productlines.csv\")\n",
        "    val productsDF      = loadCSV(spark, s\"$basePath/products.csv\")\n",
        "\n",
        "    // Create Temporary Views\n",
        "    customersDF.createOrReplaceTempView(\"customers\")\n",
        "    ordersDF.createOrReplaceTempView(\"orders\")\n",
        "    orderDetailsDF.createOrReplaceTempView(\"orderdetails\")\n",
        "    productsDF.createOrReplaceTempView(\"products\")\n",
        "    paymentsDF.createOrReplaceTempView(\"payments\")\n",
        "\n",
        "    println(\"✅ Temporary views created.\")\n",
        "\n",
        "    // Logical Data Model Tables (selected columns)\n",
        "    val productsTable = productsDF.select(\"productCode\", \"productName\", \"productLine\", \"quantityInStock\", \"buyPrice\", \"MSRP\")\n",
        "    val customersTable = customersDF.select(\"customerNumber\", \"customerName\", \"contactLastName\", \"phone\", \"city\", \"creditLimit\")\n",
        "    val ordersTable = ordersDF.select(\"orderNumber\", \"customerNumber\", \"orderDate\", \"status\", \"comments\", \"shippedDate\")\n",
        "\n",
        "    productsTable.createOrReplaceTempView(\"products_table\")\n",
        "    customersTable.createOrReplaceTempView(\"customers_table\")\n",
        "    ordersTable.createOrReplaceTempView(\"orders_table\")\n",
        "\n",
        "    println(\"✅ Logical data model created.\")\n",
        "\n",
        "    // Sample data insertion example (can be extended)\n",
        "    val sampleProducts = Seq(\n",
        "      (\"S10_1949\", \"1952 Alpine Renault\", \"Classic Cars\", 7300, 53.9, 95.7)\n",
        "    ).toDF(\"productCode\", \"productName\", \"productLine\", \"quantityInStock\", \"buyPrice\", \"MSRP\")\n",
        "\n",
        "    sampleProducts.createOrReplaceTempView(\"sample_products\")\n",
        "    println(\"\\nSample products inserted:\")\n",
        "    sampleProducts.show()\n",
        "\n",
        "    // Transformations & Aggregations\n",
        "\n",
        "    // 1. Total order value by customer\n",
        "    val totalOrderValueByCustomer = ordersDF\n",
        "      .join(orderDetailsDF, \"orderNumber\")\n",
        "      .groupBy(\"customerNumber\")\n",
        "      .agg(round(sum(col(\"priceEach\") * col(\"quantityOrdered\")), 2).alias(\"total_order_value\"))\n",
        "      .orderBy(desc(\"total_order_value\"))\n",
        "\n",
        "    println(\"\\n📊 Total Order Value by Customer:\")\n",
        "    totalOrderValueByCustomer.show(5)\n",
        "\n",
        "    // 2. Products with lowest stock per product line\n",
        "    val lowestStock = productsDF\n",
        "      .groupBy(\"productLine\")\n",
        "      .agg(min(\"quantityInStock\").alias(\"minStock\"))\n",
        "      .join(productsDF, Seq(\"productLine\"))\n",
        "      .where(col(\"quantityInStock\") === col(\"minStock\"))\n",
        "      .select(\"productCode\", \"productLine\", \"productName\", \"quantityInStock\")\n",
        "\n",
        "    println(\"\\n📉 Products with Lowest Stock:\")\n",
        "    lowestStock.show()\n",
        "\n",
        "    // 3. Top 5 customers by total payments\n",
        "    val topPayments = paymentsDF\n",
        "      .groupBy(\"customerNumber\")\n",
        "      .agg(round(sum(\"amount\"), 2).alias(\"totalPayment\"))\n",
        "      .join(customersDF, \"customerNumber\")\n",
        "      .select(\"customerNumber\", \"customerName\", \"totalPayment\")\n",
        "      .orderBy(desc(\"totalPayment\"))\n",
        "      .limit(5)\n",
        "\n",
        "    println(\"\\n💰 Top 5 Customers by Total Payment Amount:\")\n",
        "    topPayments.show()\n",
        "\n",
        "    // 4. Monthly Order Trends using Window functions\n",
        "    val monthlyTrends = ordersDF\n",
        "      .withColumn(\"month\", date_format(col(\"orderDate\"), \"yyyy-MM\"))\n",
        "      .groupBy(\"month\")\n",
        "      .agg(count(\"*\").alias(\"totalOrders\"))\n",
        "      .withColumn(\"rank\", dense_rank().over(Window.orderBy(desc(\"totalOrders\"))))\n",
        "\n",
        "    println(\"\\n📆 Monthly Order Trends:\")\n",
        "    monthlyTrends.show()\n",
        "\n",
        "    // 5. SQL query on recent shipped orders\n",
        "    val recentShippedOrders = spark.sql(\n",
        "      \"\"\"\n",
        "        |SELECT o.orderNumber, c.customerName, o.status, o.orderDate\n",
        "        |FROM orders o\n",
        "        |JOIN customers c ON o.customerNumber = c.customerNumber\n",
        "        |WHERE o.status = 'Shipped'\n",
        "        |ORDER BY o.orderDate DESC\n",
        "        |LIMIT 5\n",
        "      \"\"\".stripMargin)\n",
        "\n",
        "    println(\"\\n📄 Recent Shipped Orders:\")\n",
        "    recentShippedOrders.show()\n",
        "\n",
        "    // Validate relationships with joins (optional, can call separately)\n",
        "    validateJoins(customersDF, employeesDF, officesDF, orderDetailsDF, ordersDF, paymentsDF, productLinesDF, productsDF)\n",
        "\n",
        "    // Save outputs as CSV files\n",
        "    val outputPath = \"/content/output_csv\"\n",
        "\n",
        "    totalOrderValueByCustomer\n",
        "      .coalesce(1)\n",
        "      .write.option(\"header\", \"true\")\n",
        "      .mode(\"overwrite\")\n",
        "      .csv(s\"$outputPath/total_order_value\")\n",
        "\n",
        "    lowestStock\n",
        "      .coalesce(1)\n",
        "      .write.option(\"header\", \"true\")\n",
        "      .mode(\"overwrite\")\n",
        "      .csv(s\"$outputPath/lowest_stock\")\n",
        "\n",
        "    topPayments\n",
        "      .coalesce(1)\n",
        "      .write.option(\"header\", \"true\")\n",
        "      .mode(\"overwrite\")\n",
        "      .csv(s\"$outputPath/top_customers_by_payment\")\n",
        "\n",
        "    monthlyTrends\n",
        "      .coalesce(1)\n",
        "      .write.option(\"header\", \"true\")\n",
        "      .mode(\"overwrite\")\n",
        "      .csv(s\"$outputPath/monthly_order_trends\")\n",
        "\n",
        "    recentShippedOrders\n",
        "      .coalesce(1)\n",
        "      .write.option(\"header\", \"true\")\n",
        "      .mode(\"overwrite\")\n",
        "      .csv(s\"$outputPath/recent_shipped_orders\")\n",
        "\n",
        "    println(s\"\\n✅ All important DataFrames saved to: $outputPath\")\n",
        "\n",
        "    spark.stop()\n",
        "  }\n",
        "\n",
        "  def loadCSV(spark: SparkSession, path: String): DataFrame = {\n",
        "    spark.read\n",
        "      .option(\"header\", \"true\")\n",
        "      .option(\"inferSchema\", \"true\")\n",
        "      .csv(path)\n",
        "  }\n",
        "\n",
        "  def validateJoins(\n",
        "      customers: DataFrame,\n",
        "      employees: DataFrame,\n",
        "      offices: DataFrame,\n",
        "      orderDetails: DataFrame,\n",
        "      orders: DataFrame,\n",
        "      payments: DataFrame,\n",
        "      productLines: DataFrame,\n",
        "      products: DataFrame\n",
        "  ): Unit = {\n",
        "    println(\"\\n🔗 Validating Relationships Using Joins:\")\n",
        "\n",
        "    println(\"\\n➡️ orders JOIN customers\")\n",
        "    val ordersCustomers = orders.join(customers, \"customerNumber\")\n",
        "    ordersCustomers.select(\"orderNumber\", \"customerName\", \"status\").show(5)\n",
        "\n",
        "    println(\"\\n➡️ orderdetails JOIN orders\")\n",
        "    val orderDetailsOrders = orderDetails.join(orders, \"orderNumber\")\n",
        "    orderDetailsOrders.select(\"orderNumber\", \"productCode\", \"quantityOrdered\").show(5)\n",
        "\n",
        "    println(\"\\n➡️ employees JOIN offices\")\n",
        "    val employeesOffices = employees.join(offices, \"officeCode\")\n",
        "    employeesOffices.select(\"employeeNumber\", \"firstName\", \"officeCode\", \"city\").show(5)\n",
        "\n",
        "    println(\"\\n➡️ payments JOIN customers\")\n",
        "    val paymentsCustomers = payments.join(customers, \"customerNumber\")\n",
        "    paymentsCustomers.select(\"customerName\", \"checkNumber\", \"amount\").show(5)\n",
        "\n",
        "    println(\"\\n➡️ products JOIN productlines\")\n",
        "    val productsProductLines = products.join(productLines, \"productLine\")\n",
        "    productsProductLines.select(\"productCode\", \"productLine\", \"productName\").show(5)\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGQhSOyOYiaB",
        "outputId": "3472850d-8295-4dce-b25a-d4dd85a8d193"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ClassicModelsApp.scala\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scalac -classpath \"$SPARK_HOME/jars/*\" ClassicModelsApp.scala"
      ],
      "metadata": {
        "id": "3FX_vFjhcnkM"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!!scala -J-Xmx1g -classpath \".:$SPARK_HOME/jars/*\" ClassicModelsApp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPtpd6YRgrY9",
        "outputId": "def27349-876d-4275-8022-cf842e141081"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\",\n",
              " '25/08/04 10:47:43 INFO SparkContext: Running Spark version 3.4.1',\n",
              " '25/08/04 10:47:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
              " '25/08/04 10:47:43 INFO ResourceUtils: ==============================================================',\n",
              " '25/08/04 10:47:43 INFO ResourceUtils: No custom resources configured for spark.driver.',\n",
              " '25/08/04 10:47:43 INFO ResourceUtils: ==============================================================',\n",
              " '25/08/04 10:47:43 INFO SparkContext: Submitted application: ClassicModels Analytics',\n",
              " '25/08/04 10:47:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)',\n",
              " '25/08/04 10:47:43 INFO ResourceProfile: Limiting resource is cpu',\n",
              " '25/08/04 10:47:43 INFO ResourceProfileManager: Added ResourceProfile id: 0',\n",
              " '25/08/04 10:47:43 INFO SecurityManager: Changing view acls to: root',\n",
              " '25/08/04 10:47:43 INFO SecurityManager: Changing modify acls to: root',\n",
              " '25/08/04 10:47:43 INFO SecurityManager: Changing view acls groups to: ',\n",
              " '25/08/04 10:47:43 INFO SecurityManager: Changing modify acls groups to: ',\n",
              " '25/08/04 10:47:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY',\n",
              " \"25/08/04 10:47:44 INFO Utils: Successfully started service 'sparkDriver' on port 42165.\",\n",
              " '25/08/04 10:47:44 INFO SparkEnv: Registering MapOutputTracker',\n",
              " '25/08/04 10:47:44 INFO SparkEnv: Registering BlockManagerMaster',\n",
              " '25/08/04 10:47:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information',\n",
              " '25/08/04 10:47:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up',\n",
              " '25/08/04 10:47:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat',\n",
              " '25/08/04 10:47:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bf0029b9-d3b6-4c5f-9981-ec8bac156eae',\n",
              " '25/08/04 10:47:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB',\n",
              " '25/08/04 10:47:44 INFO SparkEnv: Registering OutputCommitCoordinator',\n",
              " '25/08/04 10:47:44 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI',\n",
              " \"25/08/04 10:47:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\",\n",
              " \"25/08/04 10:47:44 INFO Utils: Successfully started service 'SparkUI' on port 4041.\",\n",
              " '25/08/04 10:47:44 INFO Executor: Starting executor ID driver on host 4e47019e14ba',\n",
              " \"25/08/04 10:47:44 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\",\n",
              " \"25/08/04 10:47:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33693.\",\n",
              " '25/08/04 10:47:44 INFO NettyBlockTransferService: Server created on 4e47019e14ba:33693',\n",
              " '25/08/04 10:47:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy',\n",
              " '25/08/04 10:47:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4e47019e14ba, 33693, None)',\n",
              " '25/08/04 10:47:44 INFO BlockManagerMasterEndpoint: Registering block manager 4e47019e14ba:33693 with 366.3 MiB RAM, BlockManagerId(driver, 4e47019e14ba, 33693, None)',\n",
              " '25/08/04 10:47:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4e47019e14ba, 33693, None)',\n",
              " '25/08/04 10:47:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4e47019e14ba, 33693, None)',\n",
              " \"25/08/04 10:47:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\",\n",
              " \"25/08/04 10:47:45 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\",\n",
              " '25/08/04 10:47:46 INFO InMemoryFileIndex: It took 38 ms to list leaf files for 1 paths.',\n",
              " '25/08/04 10:47:46 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.',\n",
              " '25/08/04 10:47:49 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:47:49 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)',\n",
              " '25/08/04 10:47:50 INFO CodeGenerator: Code generated in 307.506667 ms',\n",
              " '25/08/04 10:47:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 349.1 KiB, free 366.0 MiB)',\n",
              " '25/08/04 10:47:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 365.9 MiB)',\n",
              " '25/08/04 10:47:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.3 MiB)',\n",
              " '25/08/04 10:47:50 INFO SparkContext: Created broadcast 0 from csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:50 INFO SparkContext: Starting job: csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:50 INFO DAGScheduler: Got job 0 (csv at ClassicModelsApp.scala:160) with 1 output partitions',\n",
              " '25/08/04 10:47:50 INFO DAGScheduler: Final stage: ResultStage 0 (csv at ClassicModelsApp.scala:160)',\n",
              " '25/08/04 10:47:50 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:50 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:50 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at ClassicModelsApp.scala:160), which has no missing parents',\n",
              " '25/08/04 10:47:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 365.9 MiB)',\n",
              " '25/08/04 10:47:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.9 MiB)',\n",
              " '25/08/04 10:47:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4e47019e14ba:33693 (size: 6.0 KiB, free: 366.3 MiB)',\n",
              " '25/08/04 10:47:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at ClassicModelsApp.scala:160) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7913 bytes) ',\n",
              " '25/08/04 10:47:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)',\n",
              " '25/08/04 10:47:51 INFO FileScanRDD: Reading File path: file:///content/csv/customers.csv, range: 0-13923, partition values: [empty row]',\n",
              " '25/08/04 10:47:51 INFO CodeGenerator: Code generated in 29.388686 ms',\n",
              " '25/08/04 10:47:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1786 bytes result sent to driver',\n",
              " '25/08/04 10:47:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 658 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:51 INFO DAGScheduler: ResultStage 0 (csv at ClassicModelsApp.scala:160) finished in 0.875 s',\n",
              " '25/08/04 10:47:51 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished',\n",
              " '25/08/04 10:47:51 INFO DAGScheduler: Job 0 finished: csv at ClassicModelsApp.scala:160, took 0.966968 s',\n",
              " '25/08/04 10:47:51 INFO CodeGenerator: Code generated in 21.991848 ms',\n",
              " '25/08/04 10:47:51 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:47:51 INFO FileSourceStrategy: Post-Scan Filters: ',\n",
              " '25/08/04 10:47:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 349.1 KiB, free 365.6 MiB)',\n",
              " '25/08/04 10:47:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 365.5 MiB)',\n",
              " '25/08/04 10:47:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:51 INFO SparkContext: Created broadcast 2 from csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:51 INFO SparkContext: Starting job: csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:51 INFO DAGScheduler: Got job 1 (csv at ClassicModelsApp.scala:160) with 1 output partitions',\n",
              " '25/08/04 10:47:51 INFO DAGScheduler: Final stage: ResultStage 1 (csv at ClassicModelsApp.scala:160)',\n",
              " '25/08/04 10:47:51 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:51 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:51 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at ClassicModelsApp.scala:160), which has no missing parents',\n",
              " '25/08/04 10:47:51 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 26.0 KiB, free 365.5 MiB)',\n",
              " '25/08/04 10:47:51 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 365.5 MiB)',\n",
              " '25/08/04 10:47:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 4e47019e14ba:33693 (size: 12.1 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:52 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at ClassicModelsApp.scala:160) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7913 bytes) ',\n",
              " '25/08/04 10:47:52 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)',\n",
              " '25/08/04 10:47:52 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 4e47019e14ba:33693 in memory (size: 6.0 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:52 INFO FileScanRDD: Reading File path: file:///content/csv/customers.csv, range: 0-13923, partition values: [empty row]',\n",
              " '25/08/04 10:47:52 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1762 bytes result sent to driver',\n",
              " '25/08/04 10:47:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 284 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: ResultStage 1 (csv at ClassicModelsApp.scala:160) finished in 0.504 s',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Job 1 finished: csv at ClassicModelsApp.scala:160, took 0.514216 s',\n",
              " '25/08/04 10:47:52 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.',\n",
              " '25/08/04 10:47:52 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.',\n",
              " '25/08/04 10:47:52 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:47:52 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#43, None)) > 0)',\n",
              " '25/08/04 10:47:52 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 349.1 KiB, free 365.2 MiB)',\n",
              " '25/08/04 10:47:52 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 365.1 MiB)',\n",
              " '25/08/04 10:47:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:52 INFO SparkContext: Created broadcast 4 from csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:52 INFO SparkContext: Starting job: csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Got job 2 (csv at ClassicModelsApp.scala:160) with 1 output partitions',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Final stage: ResultStage 2 (csv at ClassicModelsApp.scala:160)',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at csv at ClassicModelsApp.scala:160), which has no missing parents',\n",
              " '25/08/04 10:47:52 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.1 KiB, free 365.1 MiB)',\n",
              " '25/08/04 10:47:52 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.1 MiB)',\n",
              " '25/08/04 10:47:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 4e47019e14ba:33693 (size: 6.0 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:52 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at csv at ClassicModelsApp.scala:160) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:52 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:52 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7913 bytes) ',\n",
              " '25/08/04 10:47:52 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)',\n",
              " '25/08/04 10:47:52 INFO FileScanRDD: Reading File path: file:///content/csv/employees.csv, range: 0-1781, partition values: [empty row]',\n",
              " '25/08/04 10:47:52 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1689 bytes result sent to driver',\n",
              " '25/08/04 10:47:52 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 28 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:52 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: ResultStage 2 (csv at ClassicModelsApp.scala:160) finished in 0.043 s',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Job 2 finished: csv at ClassicModelsApp.scala:160, took 0.047804 s',\n",
              " '25/08/04 10:47:52 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:47:52 INFO FileSourceStrategy: Post-Scan Filters: ',\n",
              " '25/08/04 10:47:52 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 349.1 KiB, free 364.8 MiB)',\n",
              " '25/08/04 10:47:52 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 364.7 MiB)',\n",
              " '25/08/04 10:47:52 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:52 INFO SparkContext: Created broadcast 6 from csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:52 INFO SparkContext: Starting job: csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Got job 3 (csv at ClassicModelsApp.scala:160) with 1 output partitions',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Final stage: ResultStage 3 (csv at ClassicModelsApp.scala:160)',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[19] at csv at ClassicModelsApp.scala:160), which has no missing parents',\n",
              " '25/08/04 10:47:52 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 25.9 KiB, free 364.7 MiB)',\n",
              " '25/08/04 10:47:52 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 364.7 MiB)',\n",
              " '25/08/04 10:47:52 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 4e47019e14ba:33693 (size: 12.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:52 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[19] at csv at ClassicModelsApp.scala:160) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:52 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:52 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7913 bytes) ',\n",
              " '25/08/04 10:47:52 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)',\n",
              " '25/08/04 10:47:52 INFO FileScanRDD: Reading File path: file:///content/csv/employees.csv, range: 0-1781, partition values: [empty row]',\n",
              " '25/08/04 10:47:52 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1651 bytes result sent to driver',\n",
              " '25/08/04 10:47:52 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 37 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: ResultStage 3 (csv at ClassicModelsApp.scala:160) finished in 0.052 s',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:52 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Job 3 finished: csv at ClassicModelsApp.scala:160, took 0.057081 s',\n",
              " '25/08/04 10:47:52 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.',\n",
              " '25/08/04 10:47:52 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.',\n",
              " '25/08/04 10:47:52 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:47:52 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#76, None)) > 0)',\n",
              " '25/08/04 10:47:52 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 349.1 KiB, free 364.4 MiB)',\n",
              " '25/08/04 10:47:52 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 364.3 MiB)',\n",
              " '25/08/04 10:47:52 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:52 INFO SparkContext: Created broadcast 8 from csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:52 INFO SparkContext: Starting job: csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Got job 4 (csv at ClassicModelsApp.scala:160) with 1 output partitions',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Final stage: ResultStage 4 (csv at ClassicModelsApp.scala:160)',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[23] at csv at ClassicModelsApp.scala:160), which has no missing parents',\n",
              " '25/08/04 10:47:52 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 12.1 KiB, free 364.3 MiB)',\n",
              " '25/08/04 10:47:52 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 364.3 MiB)',\n",
              " '25/08/04 10:47:52 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 4e47019e14ba:33693 (size: 6.0 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:52 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[23] at csv at ClassicModelsApp.scala:160) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:52 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:52 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7911 bytes) ',\n",
              " '25/08/04 10:47:52 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)',\n",
              " '25/08/04 10:47:52 INFO FileScanRDD: Reading File path: file:///content/csv/offices.csv, range: 0-585, partition values: [empty row]',\n",
              " '25/08/04 10:47:52 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1648 bytes result sent to driver',\n",
              " '25/08/04 10:47:52 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 16 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:52 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: ResultStage 4 (csv at ClassicModelsApp.scala:160) finished in 0.028 s',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Job 4 finished: csv at ClassicModelsApp.scala:160, took 0.033669 s',\n",
              " '25/08/04 10:47:52 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:47:52 INFO FileSourceStrategy: Post-Scan Filters: ',\n",
              " '25/08/04 10:47:52 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 349.1 KiB, free 364.0 MiB)',\n",
              " '25/08/04 10:47:52 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 363.9 MiB)',\n",
              " '25/08/04 10:47:52 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:52 INFO SparkContext: Created broadcast 10 from csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:52 INFO SparkContext: Starting job: csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Got job 5 (csv at ClassicModelsApp.scala:160) with 1 output partitions',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Final stage: ResultStage 5 (csv at ClassicModelsApp.scala:160)',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:52 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at csv at ClassicModelsApp.scala:160), which has no missing parents',\n",
              " '25/08/04 10:47:53 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 26.0 KiB, free 363.9 MiB)',\n",
              " '25/08/04 10:47:53 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 363.9 MiB)',\n",
              " '25/08/04 10:47:53 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 4e47019e14ba:33693 (size: 12.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:53 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at csv at ClassicModelsApp.scala:160) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:53 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:53 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7911 bytes) ',\n",
              " '25/08/04 10:47:53 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)',\n",
              " '25/08/04 10:47:53 INFO FileScanRDD: Reading File path: file:///content/csv/offices.csv, range: 0-585, partition values: [empty row]',\n",
              " '25/08/04 10:47:53 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1646 bytes result sent to driver',\n",
              " '25/08/04 10:47:53 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 71 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:53 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: ResultStage 5 (csv at ClassicModelsApp.scala:160) finished in 0.096 s',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Job 5 finished: csv at ClassicModelsApp.scala:160, took 0.108124 s',\n",
              " '25/08/04 10:47:53 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 4e47019e14ba:33693 in memory (size: 12.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:53 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.',\n",
              " '25/08/04 10:47:53 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.',\n",
              " '25/08/04 10:47:53 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:53 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 4e47019e14ba:33693 in memory (size: 6.0 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:53 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:53 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 4e47019e14ba:33693 in memory (size: 12.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:53 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 4e47019e14ba:33693 in memory (size: 6.0 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:53 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:47:53 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#111, None)) > 0)',\n",
              " '25/08/04 10:47:53 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 349.1 KiB, free 364.4 MiB)',\n",
              " '25/08/04 10:47:53 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 364.4 MiB)',\n",
              " '25/08/04 10:47:53 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:53 INFO SparkContext: Created broadcast 12 from csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:53 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:53 INFO SparkContext: Starting job: csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Got job 6 (csv at ClassicModelsApp.scala:160) with 1 output partitions',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Final stage: ResultStage 6 (csv at ClassicModelsApp.scala:160)',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[33] at csv at ClassicModelsApp.scala:160), which has no missing parents',\n",
              " '25/08/04 10:47:53 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 12.1 KiB, free 364.8 MiB)',\n",
              " '25/08/04 10:47:53 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 364.7 MiB)',\n",
              " '25/08/04 10:47:53 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 4e47019e14ba:33693 (size: 6.0 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:53 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[33] at csv at ClassicModelsApp.scala:160) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:53 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:53 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7916 bytes) ',\n",
              " '25/08/04 10:47:53 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)',\n",
              " '25/08/04 10:47:53 INFO FileScanRDD: Reading File path: file:///content/csv/orderdetails.csv, range: 0-79703, partition values: [empty row]',\n",
              " '25/08/04 10:47:53 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1678 bytes result sent to driver',\n",
              " '25/08/04 10:47:53 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 15 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:53 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: ResultStage 6 (csv at ClassicModelsApp.scala:160) finished in 0.025 s',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Job 6 finished: csv at ClassicModelsApp.scala:160, took 0.028591 s',\n",
              " '25/08/04 10:47:53 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:47:53 INFO FileSourceStrategy: Post-Scan Filters: ',\n",
              " '25/08/04 10:47:53 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 349.1 KiB, free 364.4 MiB)',\n",
              " '25/08/04 10:47:53 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 364.4 MiB)',\n",
              " '25/08/04 10:47:53 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:53 INFO SparkContext: Created broadcast 14 from csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:53 INFO SparkContext: Starting job: csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Got job 7 (csv at ClassicModelsApp.scala:160) with 1 output partitions',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Final stage: ResultStage 7 (csv at ClassicModelsApp.scala:160)',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[39] at csv at ClassicModelsApp.scala:160), which has no missing parents',\n",
              " '25/08/04 10:47:53 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 25.9 KiB, free 364.3 MiB)',\n",
              " '25/08/04 10:47:53 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 364.3 MiB)',\n",
              " '25/08/04 10:47:53 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 4e47019e14ba:33693 (size: 12.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:53 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[39] at csv at ClassicModelsApp.scala:160) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:53 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:53 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7916 bytes) ',\n",
              " '25/08/04 10:47:53 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)',\n",
              " '25/08/04 10:47:53 INFO FileScanRDD: Reading File path: file:///content/csv/orderdetails.csv, range: 0-79703, partition values: [empty row]',\n",
              " '25/08/04 10:47:53 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1633 bytes result sent to driver',\n",
              " '25/08/04 10:47:53 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 188 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:53 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: ResultStage 7 (csv at ClassicModelsApp.scala:160) finished in 0.204 s',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Job 7 finished: csv at ClassicModelsApp.scala:160, took 0.213640 s',\n",
              " '25/08/04 10:47:53 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.',\n",
              " '25/08/04 10:47:53 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.',\n",
              " '25/08/04 10:47:53 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:47:53 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#138, None)) > 0)',\n",
              " '25/08/04 10:47:53 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 349.1 KiB, free 364.0 MiB)',\n",
              " '25/08/04 10:47:53 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 364.0 MiB)',\n",
              " '25/08/04 10:47:53 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:53 INFO SparkContext: Created broadcast 16 from csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:53 INFO SparkContext: Starting job: csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Got job 8 (csv at ClassicModelsApp.scala:160) with 1 output partitions',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Final stage: ResultStage 8 (csv at ClassicModelsApp.scala:160)',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[43] at csv at ClassicModelsApp.scala:160), which has no missing parents',\n",
              " '25/08/04 10:47:53 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 12.1 KiB, free 364.0 MiB)',\n",
              " '25/08/04 10:47:53 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 363.9 MiB)',\n",
              " '25/08/04 10:47:53 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 4e47019e14ba:33693 (size: 6.0 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:53 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[43] at csv at ClassicModelsApp.scala:160) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:53 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:53 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7910 bytes) ',\n",
              " '25/08/04 10:47:53 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)',\n",
              " '25/08/04 10:47:53 INFO FileScanRDD: Reading File path: file:///content/csv/orders.csv, range: 0-23548, partition values: [empty row]',\n",
              " '25/08/04 10:47:53 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 1636 bytes result sent to driver',\n",
              " '25/08/04 10:47:53 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 24 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:53 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: ResultStage 8 (csv at ClassicModelsApp.scala:160) finished in 0.041 s',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished',\n",
              " '25/08/04 10:47:53 INFO DAGScheduler: Job 8 finished: csv at ClassicModelsApp.scala:160, took 0.044170 s',\n",
              " '25/08/04 10:47:53 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:47:53 INFO FileSourceStrategy: Post-Scan Filters: ',\n",
              " '25/08/04 10:47:53 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 349.1 KiB, free 363.6 MiB)',\n",
              " '25/08/04 10:47:53 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 363.6 MiB)',\n",
              " '25/08/04 10:47:53 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:54 INFO SparkContext: Created broadcast 18 from csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:54 INFO SparkContext: Starting job: csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Got job 9 (csv at ClassicModelsApp.scala:160) with 1 output partitions',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Final stage: ResultStage 9 (csv at ClassicModelsApp.scala:160)',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[49] at csv at ClassicModelsApp.scala:160), which has no missing parents',\n",
              " '25/08/04 10:47:54 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 25.9 KiB, free 363.5 MiB)',\n",
              " '25/08/04 10:47:54 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 363.5 MiB)',\n",
              " '25/08/04 10:47:54 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 4e47019e14ba:33693 (size: 12.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:54 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[49] at csv at ClassicModelsApp.scala:160) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:54 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:54 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7910 bytes) ',\n",
              " '25/08/04 10:47:54 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)',\n",
              " '25/08/04 10:47:54 INFO FileScanRDD: Reading File path: file:///content/csv/orders.csv, range: 0-23548, partition values: [empty row]',\n",
              " '25/08/04 10:47:54 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 1687 bytes result sent to driver',\n",
              " '25/08/04 10:47:54 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 98 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:54 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: ResultStage 9 (csv at ClassicModelsApp.scala:160) finished in 0.118 s',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Job 9 finished: csv at ClassicModelsApp.scala:160, took 0.123167 s',\n",
              " '25/08/04 10:47:54 INFO InMemoryFileIndex: It took 17 ms to list leaf files for 1 paths.',\n",
              " '25/08/04 10:47:54 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.',\n",
              " '25/08/04 10:47:54 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:47:54 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#169, None)) > 0)',\n",
              " '25/08/04 10:47:54 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 349.1 KiB, free 363.2 MiB)',\n",
              " '25/08/04 10:47:54 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 363.2 MiB)',\n",
              " '25/08/04 10:47:54 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:54 INFO SparkContext: Created broadcast 20 from csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:54 INFO SparkContext: Starting job: csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Got job 10 (csv at ClassicModelsApp.scala:160) with 1 output partitions',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Final stage: ResultStage 10 (csv at ClassicModelsApp.scala:160)',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[53] at csv at ClassicModelsApp.scala:160), which has no missing parents',\n",
              " '25/08/04 10:47:54 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 12.1 KiB, free 363.1 MiB)',\n",
              " '25/08/04 10:47:54 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 363.1 MiB)',\n",
              " '25/08/04 10:47:54 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 4e47019e14ba:33693 (size: 6.0 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:54 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[53] at csv at ClassicModelsApp.scala:160) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:54 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:54 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes) ',\n",
              " '25/08/04 10:47:54 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)',\n",
              " '25/08/04 10:47:54 INFO FileScanRDD: Reading File path: file:///content/csv/payments.csv, range: 0-8968, partition values: [empty row]',\n",
              " '25/08/04 10:47:54 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 1614 bytes result sent to driver',\n",
              " '25/08/04 10:47:54 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 23 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:54 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: ResultStage 10 (csv at ClassicModelsApp.scala:160) finished in 0.070 s',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Job 10 finished: csv at ClassicModelsApp.scala:160, took 0.082206 s',\n",
              " '25/08/04 10:47:54 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:47:54 INFO FileSourceStrategy: Post-Scan Filters: ',\n",
              " '25/08/04 10:47:54 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 349.1 KiB, free 362.8 MiB)',\n",
              " '25/08/04 10:47:54 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:54 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 363.1 MiB)',\n",
              " '25/08/04 10:47:54 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:54 INFO SparkContext: Created broadcast 22 from csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:54 INFO SparkContext: Starting job: csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Got job 11 (csv at ClassicModelsApp.scala:160) with 1 output partitions',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Final stage: ResultStage 11 (csv at ClassicModelsApp.scala:160)',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[59] at csv at ClassicModelsApp.scala:160), which has no missing parents',\n",
              " '25/08/04 10:47:54 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 25.9 KiB, free 363.1 MiB)',\n",
              " '25/08/04 10:47:54 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 363.1 MiB)',\n",
              " '25/08/04 10:47:54 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 4e47019e14ba:33693 (size: 12.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:54 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:54 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 4e47019e14ba:33693 in memory (size: 12.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[59] at csv at ClassicModelsApp.scala:160) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:54 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:54 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes) ',\n",
              " '25/08/04 10:47:54 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)',\n",
              " '25/08/04 10:47:54 INFO FileScanRDD: Reading File path: file:///content/csv/payments.csv, range: 0-8968, partition values: [empty row]',\n",
              " '25/08/04 10:47:54 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:54 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 1679 bytes result sent to driver',\n",
              " '25/08/04 10:47:54 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 79 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:54 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: ResultStage 11 (csv at ClassicModelsApp.scala:160) finished in 0.132 s',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished',\n",
              " '25/08/04 10:47:54 INFO DAGScheduler: Job 11 finished: csv at ClassicModelsApp.scala:160, took 0.156774 s',\n",
              " '25/08/04 10:47:54 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:54 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:54 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.',\n",
              " '25/08/04 10:47:54 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 4e47019e14ba:33693 in memory (size: 6.0 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:54 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.',\n",
              " '25/08/04 10:47:54 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 4e47019e14ba:33693 in memory (size: 6.0 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:54 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 4e47019e14ba:33693 in memory (size: 6.0 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:54 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:54 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 4e47019e14ba:33693 in memory (size: 12.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:55 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 4e47019e14ba:33693 in memory (size: 12.1 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:55 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:47:55 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#194, None)) > 0)',\n",
              " '25/08/04 10:47:55 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 349.1 KiB, free 364.4 MiB)',\n",
              " '25/08/04 10:47:55 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 364.4 MiB)',\n",
              " '25/08/04 10:47:55 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:55 INFO SparkContext: Created broadcast 24 from csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:55 INFO SparkContext: Starting job: csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Got job 12 (csv at ClassicModelsApp.scala:160) with 1 output partitions',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Final stage: ResultStage 12 (csv at ClassicModelsApp.scala:160)',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[63] at csv at ClassicModelsApp.scala:160), which has no missing parents',\n",
              " '25/08/04 10:47:55 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 12.1 KiB, free 364.4 MiB)',\n",
              " '25/08/04 10:47:55 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 364.4 MiB)',\n",
              " '25/08/04 10:47:55 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 4e47019e14ba:33693 (size: 6.0 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:55 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[63] at csv at ClassicModelsApp.scala:160) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:55 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:55 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7916 bytes) ',\n",
              " '25/08/04 10:47:55 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)',\n",
              " '25/08/04 10:47:55 INFO FileScanRDD: Reading File path: file:///content/csv/productlines.csv, range: 0-3446, partition values: [empty row]',\n",
              " '25/08/04 10:47:55 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 1617 bytes result sent to driver',\n",
              " '25/08/04 10:47:55 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 31 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:55 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: ResultStage 12 (csv at ClassicModelsApp.scala:160) finished in 0.040 s',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Job 12 finished: csv at ClassicModelsApp.scala:160, took 0.044490 s',\n",
              " '25/08/04 10:47:55 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:47:55 INFO FileSourceStrategy: Post-Scan Filters: ',\n",
              " '25/08/04 10:47:55 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 349.1 KiB, free 364.0 MiB)',\n",
              " '25/08/04 10:47:55 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 364.0 MiB)',\n",
              " '25/08/04 10:47:55 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:55 INFO SparkContext: Created broadcast 26 from csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:55 INFO SparkContext: Starting job: csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Got job 13 (csv at ClassicModelsApp.scala:160) with 1 output partitions',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Final stage: ResultStage 13 (csv at ClassicModelsApp.scala:160)',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[69] at csv at ClassicModelsApp.scala:160), which has no missing parents',\n",
              " '25/08/04 10:47:55 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 25.9 KiB, free 364.0 MiB)',\n",
              " '25/08/04 10:47:55 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 364.0 MiB)',\n",
              " '25/08/04 10:47:55 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 4e47019e14ba:33693 (size: 12.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:55 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[69] at csv at ClassicModelsApp.scala:160) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:55 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:55 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7916 bytes) ',\n",
              " '25/08/04 10:47:55 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)',\n",
              " '25/08/04 10:47:55 INFO FileScanRDD: Reading File path: file:///content/csv/productlines.csv, range: 0-3446, partition values: [empty row]',\n",
              " '25/08/04 10:47:55 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 1575 bytes result sent to driver',\n",
              " '25/08/04 10:47:55 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 55 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:55 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: ResultStage 13 (csv at ClassicModelsApp.scala:160) finished in 0.097 s',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Job 13 finished: csv at ClassicModelsApp.scala:160, took 0.104254 s',\n",
              " '25/08/04 10:47:55 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.',\n",
              " '25/08/04 10:47:55 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.',\n",
              " '25/08/04 10:47:55 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:47:55 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#219, None)) > 0)',\n",
              " '25/08/04 10:47:55 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 349.1 KiB, free 363.6 MiB)',\n",
              " '25/08/04 10:47:55 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 363.6 MiB)',\n",
              " '25/08/04 10:47:55 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:55 INFO SparkContext: Created broadcast 28 from csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:55 INFO SparkContext: Starting job: csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Got job 14 (csv at ClassicModelsApp.scala:160) with 1 output partitions',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Final stage: ResultStage 14 (csv at ClassicModelsApp.scala:160)',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[73] at csv at ClassicModelsApp.scala:160), which has no missing parents',\n",
              " '25/08/04 10:47:55 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 12.1 KiB, free 363.6 MiB)',\n",
              " '25/08/04 10:47:55 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 363.6 MiB)',\n",
              " '25/08/04 10:47:55 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 4e47019e14ba:33693 (size: 6.0 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:55 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[73] at csv at ClassicModelsApp.scala:160) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:55 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:55 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes) ',\n",
              " '25/08/04 10:47:55 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)',\n",
              " '25/08/04 10:47:55 INFO FileScanRDD: Reading File path: file:///content/csv/products.csv, range: 0-29309, partition values: [empty row]',\n",
              " '25/08/04 10:47:55 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 1655 bytes result sent to driver',\n",
              " '25/08/04 10:47:55 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 20 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:55 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: ResultStage 14 (csv at ClassicModelsApp.scala:160) finished in 0.036 s',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Job 14 finished: csv at ClassicModelsApp.scala:160, took 0.046431 s',\n",
              " '25/08/04 10:47:55 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:47:55 INFO FileSourceStrategy: Post-Scan Filters: ',\n",
              " '25/08/04 10:47:55 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 349.1 KiB, free 363.2 MiB)',\n",
              " '25/08/04 10:47:55 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 363.2 MiB)',\n",
              " '25/08/04 10:47:55 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:55 INFO SparkContext: Created broadcast 30 from csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:55 INFO SparkContext: Starting job: csv at ClassicModelsApp.scala:160',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Got job 15 (csv at ClassicModelsApp.scala:160) with 1 output partitions',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Final stage: ResultStage 15 (csv at ClassicModelsApp.scala:160)',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[79] at csv at ClassicModelsApp.scala:160), which has no missing parents',\n",
              " '25/08/04 10:47:55 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 26.0 KiB, free 363.2 MiB)',\n",
              " '25/08/04 10:47:55 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 363.2 MiB)',\n",
              " '25/08/04 10:47:55 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 4e47019e14ba:33693 (size: 12.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:55 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[79] at csv at ClassicModelsApp.scala:160) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:55 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:55 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes) ',\n",
              " '25/08/04 10:47:55 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)',\n",
              " '25/08/04 10:47:55 INFO FileScanRDD: Reading File path: file:///content/csv/products.csv, range: 0-29309, partition values: [empty row]',\n",
              " '25/08/04 10:47:55 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 1552 bytes result sent to driver',\n",
              " '25/08/04 10:47:55 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 39 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:55 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: ResultStage 15 (csv at ClassicModelsApp.scala:160) finished in 0.053 s',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished',\n",
              " '25/08/04 10:47:55 INFO DAGScheduler: Job 15 finished: csv at ClassicModelsApp.scala:160, took 0.060676 s',\n",
              " '✅ Temporary views created.',\n",
              " '✅ Logical data model created.',\n",
              " '25/08/04 10:47:56 INFO CodeGenerator: Code generated in 63.687863 ms',\n",
              " '',\n",
              " 'Sample products inserted:',\n",
              " '25/08/04 10:47:56 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:56 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 4e47019e14ba:33693 in memory (size: 6.0 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:56 INFO CodeGenerator: Code generated in 22.232157 ms',\n",
              " '25/08/04 10:47:56 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 4e47019e14ba:33693 in memory (size: 6.0 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:56 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:56 INFO CodeGenerator: Code generated in 33.606276 ms',\n",
              " '+-----------+-------------------+------------+---------------+--------+----+',\n",
              " '|productCode|        productName| productLine|quantityInStock|buyPrice|MSRP|',\n",
              " '+-----------+-------------------+------------+---------------+--------+----+',\n",
              " '|   S10_1949|1952 Alpine Renault|Classic Cars|           7300|    53.9|95.7|',\n",
              " '+-----------+-------------------+------------+---------------+--------+----+',\n",
              " '',\n",
              " '25/08/04 10:47:56 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:56 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:56 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:56 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 4e47019e14ba:33693 in memory (size: 12.1 KiB, free: 366.2 MiB)',\n",
              " '',\n",
              " '📊 Total Order Value by Customer:',\n",
              " '25/08/04 10:47:56 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 4e47019e14ba:33693 in memory (size: 12.1 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:56 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:56 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 4e47019e14ba:33693 in memory (size: 12.1 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:56 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)',\n",
              " '25/08/04 10:47:56 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#155)',\n",
              " '25/08/04 10:47:56 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)',\n",
              " '25/08/04 10:47:56 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#128)',\n",
              " '25/08/04 10:47:56 INFO CodeGenerator: Code generated in 27.986087 ms',\n",
              " '25/08/04 10:47:56 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 348.9 KiB, free 365.2 MiB)',\n",
              " '25/08/04 10:47:56 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 365.2 MiB)',\n",
              " '25/08/04 10:47:56 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:56 INFO SparkContext: Created broadcast 32 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266',\n",
              " '25/08/04 10:47:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:56 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266',\n",
              " '25/08/04 10:47:56 INFO DAGScheduler: Got job 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions',\n",
              " '25/08/04 10:47:56 INFO DAGScheduler: Final stage: ResultStage 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)',\n",
              " '25/08/04 10:47:56 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:56 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:56 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[83] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents',\n",
              " '25/08/04 10:47:56 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 14.3 KiB, free 365.2 MiB)',\n",
              " '25/08/04 10:47:56 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 365.2 MiB)',\n",
              " '25/08/04 10:47:56 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 4e47019e14ba:33693 (size: 7.3 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:56 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[83] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:56 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:56 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7910 bytes) ',\n",
              " '25/08/04 10:47:56 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)',\n",
              " '25/08/04 10:47:56 INFO FileScanRDD: Reading File path: file:///content/csv/orders.csv, range: 0-23548, partition values: [empty row]',\n",
              " '25/08/04 10:47:56 INFO CodeGenerator: Code generated in 13.451309 ms',\n",
              " '25/08/04 10:47:56 INFO CodeGenerator: Code generated in 13.846219 ms',\n",
              " '25/08/04 10:47:57 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 4577 bytes result sent to driver',\n",
              " '25/08/04 10:47:57 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 152 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:57 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:57 INFO DAGScheduler: ResultStage 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0.158 s',\n",
              " '25/08/04 10:47:57 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished',\n",
              " '25/08/04 10:47:57 INFO DAGScheduler: Job 16 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0.161679 s',\n",
              " '25/08/04 10:47:57 INFO CodeGenerator: Code generated in 46.220025 ms',\n",
              " '25/08/04 10:47:57 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 1026.6 KiB, free 364.2 MiB)',\n",
              " '25/08/04 10:47:57 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 364.1 MiB)',\n",
              " '25/08/04 10:47:57 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 4e47019e14ba:33693 (size: 4.4 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:57 INFO SparkContext: Created broadcast 34 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266',\n",
              " '25/08/04 10:47:57 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)',\n",
              " '25/08/04 10:47:57 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#128)',\n",
              " '25/08/04 10:47:57 INFO CodeGenerator: Code generated in 81.184572 ms',\n",
              " '25/08/04 10:47:57 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 348.9 KiB, free 363.8 MiB)',\n",
              " '25/08/04 10:47:57 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 363.8 MiB)',\n",
              " '25/08/04 10:47:57 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:57 INFO SparkContext: Created broadcast 35 from show at ClassicModelsApp.scala:66',\n",
              " '25/08/04 10:47:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:57 INFO DAGScheduler: Registering RDD 87 (show at ClassicModelsApp.scala:66) as input to shuffle 0',\n",
              " '25/08/04 10:47:57 INFO DAGScheduler: Got map stage job 17 (show at ClassicModelsApp.scala:66) with 1 output partitions',\n",
              " '25/08/04 10:47:57 INFO DAGScheduler: Final stage: ShuffleMapStage 17 (show at ClassicModelsApp.scala:66)',\n",
              " '25/08/04 10:47:57 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:57 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:57 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[87] at show at ClassicModelsApp.scala:66), which has no missing parents',\n",
              " '25/08/04 10:47:57 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 48.9 KiB, free 363.7 MiB)',\n",
              " '25/08/04 10:47:57 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 22.8 KiB, free 363.7 MiB)',\n",
              " '25/08/04 10:47:57 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 4e47019e14ba:33693 (size: 22.8 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:57 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:57 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[87] at show at ClassicModelsApp.scala:66) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:57 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:57 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7905 bytes) ',\n",
              " '25/08/04 10:47:57 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)',\n",
              " '25/08/04 10:47:57 INFO CodeGenerator: Code generated in 28.220987 ms',\n",
              " '25/08/04 10:47:57 INFO CodeGenerator: Code generated in 12.204698 ms',\n",
              " '25/08/04 10:47:57 INFO CodeGenerator: Code generated in 14.754738 ms',\n",
              " '25/08/04 10:47:57 INFO CodeGenerator: Code generated in 13.881409 ms',\n",
              " '25/08/04 10:47:57 INFO FileScanRDD: Reading File path: file:///content/csv/orderdetails.csv, range: 0-79703, partition values: [empty row]',\n",
              " '25/08/04 10:47:57 INFO CodeGenerator: Code generated in 31.049417 ms',\n",
              " '25/08/04 10:47:58 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 3550 bytes result sent to driver',\n",
              " '25/08/04 10:47:58 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 455 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:58 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: ShuffleMapStage 17 (show at ClassicModelsApp.scala:66) finished in 0.481 s',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: looking for newly runnable stages',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: running: Set()',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: waiting: Set()',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: failed: Set()',\n",
              " '25/08/04 10:47:58 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576',\n",
              " '25/08/04 10:47:58 INFO CodeGenerator: Code generated in 26.062308 ms',\n",
              " '25/08/04 10:47:58 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.',\n",
              " '25/08/04 10:47:58 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 4e47019e14ba:33693 in memory (size: 22.8 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:58 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 4e47019e14ba:33693 in memory (size: 7.3 KiB, free: 366.2 MiB)',\n",
              " '25/08/04 10:47:58 INFO CodeGenerator: Code generated in 48.253865 ms',\n",
              " '25/08/04 10:47:58 INFO SparkContext: Starting job: show at ClassicModelsApp.scala:66',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: Got job 18 (show at ClassicModelsApp.scala:66) with 1 output partitions',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: Final stage: ResultStage 19 (show at ClassicModelsApp.scala:66)',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[91] at show at ClassicModelsApp.scala:66), which has no missing parents',\n",
              " '25/08/04 10:47:58 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 51.5 KiB, free 363.7 MiB)',\n",
              " '25/08/04 10:47:58 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 363.7 MiB)',\n",
              " '25/08/04 10:47:58 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 4e47019e14ba:33693 (size: 24.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:58 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[91] at show at ClassicModelsApp.scala:66) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:58 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:58 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 18) (4e47019e14ba, executor driver, partition 0, NODE_LOCAL, 7363 bytes) ',\n",
              " '25/08/04 10:47:58 INFO Executor: Running task 0.0 in stage 19.0 (TID 18)',\n",
              " '25/08/04 10:47:58 INFO ShuffleBlockFetcherIterator: Getting 1 (5.3 KiB) non-empty blocks including 1 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks',\n",
              " '25/08/04 10:47:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms',\n",
              " '25/08/04 10:47:58 INFO Executor: Finished task 0.0 in stage 19.0 (TID 18). 6100 bytes result sent to driver',\n",
              " '25/08/04 10:47:58 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 18) in 106 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:58 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: ResultStage 19 (show at ClassicModelsApp.scala:66) finished in 0.129 s',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: Job 18 finished: show at ClassicModelsApp.scala:66, took 0.153204 s',\n",
              " '25/08/04 10:47:58 INFO CodeGenerator: Code generated in 9.847359 ms',\n",
              " '25/08/04 10:47:58 INFO CodeGenerator: Code generated in 14.437569 ms',\n",
              " '+--------------+-----------------+',\n",
              " '|customerNumber|total_order_value|',\n",
              " '+--------------+-----------------+',\n",
              " '|           141|        820689.54|',\n",
              " '|           124|        591827.34|',\n",
              " '|           114|        180585.07|',\n",
              " '|           151|        177913.95|',\n",
              " '|           119|        158573.12|',\n",
              " '+--------------+-----------------+',\n",
              " 'only showing top 5 rows',\n",
              " '',\n",
              " '',\n",
              " '📉 Products with Lowest Stock:',\n",
              " '25/08/04 10:47:58 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productLine)',\n",
              " '25/08/04 10:47:58 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productLine#238)',\n",
              " '25/08/04 10:47:58 INFO FileSourceStrategy: Pushed Filters: IsNotNull(quantityInStock),IsNotNull(productLine)',\n",
              " '25/08/04 10:47:58 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(quantityInStock#396),isnotnull(productLine#392)',\n",
              " '25/08/04 10:47:58 INFO CodeGenerator: Code generated in 17.617348 ms',\n",
              " '25/08/04 10:47:58 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 348.9 KiB, free 363.4 MiB)',\n",
              " '25/08/04 10:47:58 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 363.3 MiB)',\n",
              " '25/08/04 10:47:58 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:58 INFO SparkContext: Created broadcast 38 from show at ClassicModelsApp.scala:77',\n",
              " '25/08/04 10:47:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: Registering RDD 96 (show at ClassicModelsApp.scala:77) as input to shuffle 1',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: Got map stage job 19 (show at ClassicModelsApp.scala:77) with 1 output partitions',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: Final stage: ShuffleMapStage 20 (show at ClassicModelsApp.scala:77)',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[96] at show at ClassicModelsApp.scala:77), which has no missing parents',\n",
              " '25/08/04 10:47:58 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 29.8 KiB, free 363.3 MiB)',\n",
              " '25/08/04 10:47:58 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 363.3 MiB)',\n",
              " '25/08/04 10:47:58 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 4e47019e14ba:33693 (size: 14.3 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:58 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[96] at show at ClassicModelsApp.scala:77) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:58 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:58 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 19) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7901 bytes) ',\n",
              " '25/08/04 10:47:58 INFO Executor: Running task 0.0 in stage 20.0 (TID 19)',\n",
              " '25/08/04 10:47:58 INFO CodeGenerator: Code generated in 11.639559 ms',\n",
              " '25/08/04 10:47:58 INFO CodeGenerator: Code generated in 15.536048 ms',\n",
              " '25/08/04 10:47:58 INFO FileScanRDD: Reading File path: file:///content/csv/products.csv, range: 0-29309, partition values: [empty row]',\n",
              " '25/08/04 10:47:58 INFO CodeGenerator: Code generated in 23.253648 ms',\n",
              " '25/08/04 10:47:58 INFO CodeGenerator: Code generated in 6.699809 ms',\n",
              " '25/08/04 10:47:58 INFO CodeGenerator: Code generated in 9.777229 ms',\n",
              " '25/08/04 10:47:58 INFO CodeGenerator: Code generated in 12.229579 ms',\n",
              " '25/08/04 10:47:58 INFO CodeGenerator: Code generated in 7.989359 ms',\n",
              " '25/08/04 10:47:58 INFO CodeGenerator: Code generated in 12.974138 ms',\n",
              " '25/08/04 10:47:58 INFO Executor: Finished task 0.0 in stage 20.0 (TID 19). 2585 bytes result sent to driver',\n",
              " '25/08/04 10:47:58 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 19) in 256 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:58 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: ShuffleMapStage 20 (show at ClassicModelsApp.scala:77) finished in 0.270 s',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: looking for newly runnable stages',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: running: Set()',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: waiting: Set()',\n",
              " '25/08/04 10:47:58 INFO DAGScheduler: failed: Set()',\n",
              " '25/08/04 10:47:58 INFO FileSourceStrategy: Pushed Filters: IsNotNull(quantityInStock),IsNotNull(productLine)',\n",
              " '25/08/04 10:47:58 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(quantityInStock#396),isnotnull(productLine#392)',\n",
              " '25/08/04 10:47:58 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 8.260619 ms',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 13.781689 ms',\n",
              " '25/08/04 10:47:59 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Got job 20 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Final stage: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[101] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 37.2 KiB, free 363.3 MiB)',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 18.0 KiB, free 363.3 MiB)',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 4e47019e14ba:33693 (size: 18.0 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:59 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[101] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:59 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:59 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 20) (4e47019e14ba, executor driver, partition 0, NODE_LOCAL, 7363 bytes) ',\n",
              " '25/08/04 10:47:59 INFO Executor: Running task 0.0 in stage 22.0 (TID 20)',\n",
              " '25/08/04 10:47:59 INFO ShuffleBlockFetcherIterator: Getting 1 (688.0 B) non-empty blocks including 1 (688.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks',\n",
              " '25/08/04 10:47:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 14.095389 ms',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 14.842058 ms',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 11.815878 ms',\n",
              " '25/08/04 10:47:59 INFO Executor: Finished task 0.0 in stage 22.0 (TID 20). 5225 bytes result sent to driver',\n",
              " '25/08/04 10:47:59 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 20) in 75 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:59 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0.082 s',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Job 20 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0.091408 s',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 9.777139 ms',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 8.0 MiB, free 355.3 MiB)',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 540.0 B, free 355.3 MiB)',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 4e47019e14ba:33693 (size: 540.0 B, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:59 INFO SparkContext: Created broadcast 41 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266',\n",
              " '25/08/04 10:47:59 INFO FileSourceStrategy: Pushed Filters: IsNotNull(quantityInStock),IsNotNull(productLine)',\n",
              " '25/08/04 10:47:59 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(quantityInStock#396),isnotnull(productLine#392)',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 15.189318 ms',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 348.9 KiB, free 354.9 MiB)',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 354.9 MiB)',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:59 INFO SparkContext: Created broadcast 42 from show at ClassicModelsApp.scala:77',\n",
              " '25/08/04 10:47:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:59 INFO SparkContext: Starting job: show at ClassicModelsApp.scala:77',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Got job 21 (show at ClassicModelsApp.scala:77) with 1 output partitions',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Final stage: ResultStage 23 (show at ClassicModelsApp.scala:77)',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[105] at show at ClassicModelsApp.scala:77), which has no missing parents',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 16.9 KiB, free 354.9 MiB)',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 354.9 MiB)',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 4e47019e14ba:33693 (size: 8.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:59 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[105] at show at ClassicModelsApp.scala:77) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:59 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:59 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 21) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes) ',\n",
              " '25/08/04 10:47:59 INFO Executor: Running task 0.0 in stage 23.0 (TID 21)',\n",
              " '25/08/04 10:47:59 INFO FileScanRDD: Reading File path: file:///content/csv/products.csv, range: 0-29309, partition values: [empty row]',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 11.280089 ms',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 5.372189 ms',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 12.247059 ms',\n",
              " '25/08/04 10:47:59 INFO Executor: Finished task 0.0 in stage 23.0 (TID 21). 2210 bytes result sent to driver',\n",
              " '25/08/04 10:47:59 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 21) in 57 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:59 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: ResultStage 23 (show at ClassicModelsApp.scala:77) finished in 0.064 s',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Job 21 finished: show at ClassicModelsApp.scala:77, took 0.069634 s',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 12.426389 ms',\n",
              " '+-----------+----------------+--------------------+------------------+',\n",
              " '|productCode|     productLine|         productName|   quantityInStock|',\n",
              " '+-----------+----------------+--------------------+------------------+',\n",
              " '|   S12_3148|    Classic Cars|  1969 Corvair Monza|        hood opens|',\n",
              " '|   S12_4473|Trucks and Buses|   1957 Chevy Pickup|    Rubber wheels\"|',\n",
              " '|   S24_2000|     Motorcycles|1960 BSA Gold Sta...|                15|',\n",
              " '|   S24_4278|          Planes|1900s Vintage Tri...|              2756|',\n",
              " '|   S32_4289|    Vintage Cars|1928 Ford Phaeton...|               136|',\n",
              " '|   S50_1514|          Trains|1962 City of Detr...|              1645|',\n",
              " '|  S700_2610|           Ships|The USS Constitut...| sea sprite on bow|',\n",
              " '+-----------+----------------+--------------------+------------------+',\n",
              " '',\n",
              " '',\n",
              " '💰 Top 5 Customers by Total Payment Amount:',\n",
              " '25/08/04 10:47:59 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)',\n",
              " '25/08/04 10:47:59 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#186)',\n",
              " '25/08/04 10:47:59 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)',\n",
              " '25/08/04 10:47:59 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#17)',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 8.220279 ms',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 348.9 KiB, free 354.5 MiB)',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 59.576334 ms',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 348.9 KiB, free 354.2 MiB)',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 354.1 MiB)',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:59 INFO SparkContext: Created broadcast 44 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 354.1 MiB)',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:59 INFO SparkContext: Created broadcast 45 from show at ClassicModelsApp.scala:89',\n",
              " '25/08/04 10:47:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 4e47019e14ba:33693 in memory (size: 8.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Registering RDD 113 (show at ClassicModelsApp.scala:89) as input to shuffle 2',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Got map stage job 22 (show at ClassicModelsApp.scala:89) with 1 output partitions',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Final stage: ShuffleMapStage 24 (show at ClassicModelsApp.scala:89)',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[113] at show at ClassicModelsApp.scala:89), which has no missing parents',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 39.9 KiB, free 354.1 MiB)',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 18.8 KiB, free 354.1 MiB)',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 4e47019e14ba:33693 (size: 18.8 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:59 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[113] at show at ClassicModelsApp.scala:89) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:59 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:59 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 22) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7901 bytes) ',\n",
              " '25/08/04 10:47:59 INFO Executor: Running task 0.0 in stage 24.0 (TID 22)',\n",
              " '25/08/04 10:47:59 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Got job 23 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Final stage: ResultStage 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[112] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 14.5 KiB, free 354.1 MiB)',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 354.0 MiB)',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 4e47019e14ba:33693 (size: 7.4 KiB, free: 365.9 MiB)',\n",
              " '25/08/04 10:47:59 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[112] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:47:59 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:47:59 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 23) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7913 bytes) ',\n",
              " '25/08/04 10:47:59 INFO FileScanRDD: Reading File path: file:///content/csv/payments.csv, range: 0-8968, partition values: [empty row]',\n",
              " '25/08/04 10:47:59 INFO Executor: Running task 0.0 in stage 25.0 (TID 23)',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:59 INFO FileScanRDD: Reading File path: file:///content/csv/customers.csv, range: 0-13923, partition values: [empty row]',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 6.68191 ms',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 22.264508 ms',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 4e47019e14ba:33693 in memory (size: 540.0 B, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:59 INFO Executor: Finished task 0.0 in stage 25.0 (TID 23). 4929 bytes result sent to driver',\n",
              " '25/08/04 10:47:59 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 23) in 106 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:59 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: ResultStage 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0.121 s',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:47:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: Job 23 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0.132608 s',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 1027.1 KiB, free 361.8 MiB)',\n",
              " '25/08/04 10:47:59 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 361.8 MiB)',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 4e47019e14ba:33693 (size: 4.2 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:59 INFO SparkContext: Created broadcast 48 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 4e47019e14ba:33693 in memory (size: 18.0 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:47:59 INFO Executor: Finished task 0.0 in stage 24.0 (TID 22). 2778 bytes result sent to driver',\n",
              " '25/08/04 10:47:59 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 22) in 171 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:47:59 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 4e47019e14ba:33693 in memory (size: 24.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: ShuffleMapStage 24 (show at ClassicModelsApp.scala:89) finished in 0.184 s',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: looking for newly runnable stages',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: running: Set()',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: waiting: Set()',\n",
              " '25/08/04 10:47:59 INFO DAGScheduler: failed: Set()',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 4e47019e14ba:33693 in memory (size: 14.3 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:59 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 8.791919 ms',\n",
              " '25/08/04 10:47:59 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 4e47019e14ba:33693 in memory (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:59 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 4e47019e14ba:33693 in memory (size: 4.4 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:47:59 INFO CodeGenerator: Code generated in 41.819845 ms',\n",
              " '25/08/04 10:48:00 INFO SparkContext: Starting job: show at ClassicModelsApp.scala:89',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Got job 24 (show at ClassicModelsApp.scala:89) with 1 output partitions',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Final stage: ResultStage 27 (show at ClassicModelsApp.scala:89)',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[117] at show at ClassicModelsApp.scala:89), which has no missing parents',\n",
              " '25/08/04 10:48:00 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 52.3 KiB, free 363.7 MiB)',\n",
              " '25/08/04 10:48:00 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 24.6 KiB, free 363.6 MiB)',\n",
              " '25/08/04 10:48:00 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 4e47019e14ba:33693 (size: 24.6 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:48:00 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[117] at show at ClassicModelsApp.scala:89) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:48:00 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:48:00 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 24) (4e47019e14ba, executor driver, partition 0, NODE_LOCAL, 7363 bytes) ',\n",
              " '25/08/04 10:48:00 INFO Executor: Running task 0.0 in stage 27.0 (TID 24)',\n",
              " '25/08/04 10:48:00 INFO ShuffleBlockFetcherIterator: Getting 1 (5.3 KiB) non-empty blocks including 1 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks',\n",
              " '25/08/04 10:48:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms',\n",
              " '25/08/04 10:48:00 INFO Executor: Finished task 0.0 in stage 27.0 (TID 24). 6235 bytes result sent to driver',\n",
              " '25/08/04 10:48:00 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 24) in 42 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:48:00 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: ResultStage 27 (show at ClassicModelsApp.scala:89) finished in 0.051 s',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:48:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Job 24 finished: show at ClassicModelsApp.scala:89, took 0.063097 s',\n",
              " '25/08/04 10:48:00 INFO CodeGenerator: Code generated in 7.912759 ms',\n",
              " '25/08/04 10:48:00 INFO CodeGenerator: Code generated in 6.217819 ms',\n",
              " '+--------------+--------------------+------------+',\n",
              " '|customerNumber|        customerName|totalPayment|',\n",
              " '+--------------+--------------------+------------+',\n",
              " '|           141|Euro+ Shopping Ch...|   715738.98|',\n",
              " '|           124|Mini Gifts Distri...|   584188.24|',\n",
              " '|           114|Australian Collec...|   180585.07|',\n",
              " '|           151|  Muscle Machine Inc|   177913.95|',\n",
              " '|           148|Dragon Souveniers...|   156251.03|',\n",
              " '+--------------+--------------------+------------+',\n",
              " '',\n",
              " '',\n",
              " '📆 Monthly Order Trends:',\n",
              " '25/08/04 10:48:00 INFO FileSourceStrategy: Pushed Filters: ',\n",
              " '25/08/04 10:48:00 INFO FileSourceStrategy: Post-Scan Filters: ',\n",
              " '25/08/04 10:48:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.',\n",
              " '25/08/04 10:48:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.',\n",
              " '25/08/04 10:48:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.',\n",
              " '25/08/04 10:48:00 INFO CodeGenerator: Code generated in 29.721706 ms',\n",
              " '25/08/04 10:48:00 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 348.9 KiB, free 363.3 MiB)',\n",
              " '25/08/04 10:48:00 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 363.3 MiB)',\n",
              " '25/08/04 10:48:00 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 4e47019e14ba:33693 (size: 34.1 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:48:00 INFO SparkContext: Created broadcast 50 from show at ClassicModelsApp.scala:99',\n",
              " '25/08/04 10:48:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Registering RDD 121 (show at ClassicModelsApp.scala:99) as input to shuffle 3',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Got map stage job 25 (show at ClassicModelsApp.scala:99) with 1 output partitions',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Final stage: ShuffleMapStage 28 (show at ClassicModelsApp.scala:99)',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Parents of final stage: List()',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[121] at show at ClassicModelsApp.scala:99), which has no missing parents',\n",
              " '25/08/04 10:48:00 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 39.8 KiB, free 363.2 MiB)',\n",
              " '25/08/04 10:48:00 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 363.2 MiB)',\n",
              " '25/08/04 10:48:00 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 4e47019e14ba:33693 (size: 19.2 KiB, free: 366.1 MiB)',\n",
              " '25/08/04 10:48:00 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[121] at show at ClassicModelsApp.scala:99) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:48:00 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:48:00 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 25) (4e47019e14ba, executor driver, partition 0, PROCESS_LOCAL, 7899 bytes) ',\n",
              " '25/08/04 10:48:00 INFO Executor: Running task 0.0 in stage 28.0 (TID 25)',\n",
              " '25/08/04 10:48:00 INFO CodeGenerator: Code generated in 6.43511 ms',\n",
              " '25/08/04 10:48:00 INFO CodeGenerator: Code generated in 11.391099 ms',\n",
              " '25/08/04 10:48:00 INFO FileScanRDD: Reading File path: file:///content/csv/orders.csv, range: 0-23548, partition values: [empty row]',\n",
              " '25/08/04 10:48:00 INFO Executor: Finished task 0.0 in stage 28.0 (TID 25). 2722 bytes result sent to driver',\n",
              " '25/08/04 10:48:00 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 25) in 129 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:48:00 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: ShuffleMapStage 28 (show at ClassicModelsApp.scala:99) finished in 0.143 s',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: looking for newly runnable stages',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: running: Set()',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: waiting: Set()',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: failed: Set()',\n",
              " '25/08/04 10:48:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.',\n",
              " '25/08/04 10:48:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.',\n",
              " '25/08/04 10:48:00 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576',\n",
              " '25/08/04 10:48:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.',\n",
              " '25/08/04 10:48:00 INFO CodeGenerator: Code generated in 14.552038 ms',\n",
              " '25/08/04 10:48:00 INFO CodeGenerator: Code generated in 5.70748 ms',\n",
              " '25/08/04 10:48:00 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.',\n",
              " '25/08/04 10:48:00 INFO CodeGenerator: Code generated in 13.162629 ms',\n",
              " '25/08/04 10:48:00 INFO SparkContext: Starting job: show at ClassicModelsApp.scala:99',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Got job 26 (show at ClassicModelsApp.scala:99) with 1 output partitions',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Final stage: ResultStage 30 (show at ClassicModelsApp.scala:99)',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 29)',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Missing parents: List()',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[127] at show at ClassicModelsApp.scala:99), which has no missing parents',\n",
              " '25/08/04 10:48:00 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 50.9 KiB, free 363.2 MiB)',\n",
              " '25/08/04 10:48:00 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 23.6 KiB, free 363.1 MiB)',\n",
              " '25/08/04 10:48:00 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 4e47019e14ba:33693 (size: 23.6 KiB, free: 366.0 MiB)',\n",
              " '25/08/04 10:48:00 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1535',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[127] at show at ClassicModelsApp.scala:99) (first 15 tasks are for partitions Vector(0))',\n",
              " '25/08/04 10:48:00 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0',\n",
              " '25/08/04 10:48:00 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 26) (4e47019e14ba, executor driver, partition 0, NODE_LOCAL, 7363 bytes) ',\n",
              " '25/08/04 10:48:00 INFO Executor: Running task 0.0 in stage 30.0 (TID 26)',\n",
              " '25/08/04 10:48:00 INFO ShuffleBlockFetcherIterator: Getting 1 (2.0 KiB) non-empty blocks including 1 (2.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks',\n",
              " '25/08/04 10:48:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms',\n",
              " '25/08/04 10:48:00 INFO CodeGenerator: Code generated in 6.60615 ms',\n",
              " '25/08/04 10:48:00 INFO CodeGenerator: Code generated in 5.204139 ms',\n",
              " '25/08/04 10:48:00 INFO CodeGenerator: Code generated in 6.202329 ms',\n",
              " '25/08/04 10:48:00 INFO CodeGenerator: Code generated in 18.869718 ms',\n",
              " '25/08/04 10:48:00 INFO CodeGenerator: Code generated in 18.730328 ms',\n",
              " '25/08/04 10:48:00 INFO Executor: Finished task 0.0 in stage 30.0 (TID 26). 6648 bytes result sent to driver',\n",
              " '25/08/04 10:48:00 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 26) in 172 ms on 4e47019e14ba (executor driver) (1/1)',\n",
              " '25/08/04 10:48:00 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool ',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: ResultStage 30 (show at ClassicModelsApp.scala:99) finished in 0.190 s',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job',\n",
              " '25/08/04 10:48:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished',\n",
              " '25/08/04 10:48:00 INFO DAGScheduler: Job 26 finished: show at ClassicModelsApp.scala:99, took 0.202477 s',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IjMVGRmnr8OO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}